{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 日産コンテスト 画像認識"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "名前：ク　レンイ\n",
    "\n",
    "学部：経済学部PEARLプログラム\n",
    "\n",
    "学年：３"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手順１：事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### パッケージのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import Augmentor\n",
    "import glob\n",
    "import random\n",
    "import keras\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import splitfolders\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### パラメーターの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "w = 1920\n",
    "h = 1080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手順２：画像の読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JupyterHubでやってみましたが、色々な問題が出ました。\n",
    "\n",
    "以下のように、駐車場の画像数と街の画像数を受けました。\n",
    "\n",
    "```{python}\n",
    "parking_list = list(map(Image.open, glob.glob('train/parking/*.png')))\n",
    "street_list = list(map(Image.open, glob.glob('train/street/*.png')))\n",
    "print('Number of Parking Images: ',len(parking_list))\n",
    "print('Number of Street Images:  ',len(street_list))\n",
    "print('Shape of each image:      ',np.array(parking_list[11]).shape)\n",
    "```\n",
    "\n",
    "```\n",
    "Number of Parking Images:  3346\n",
    "Number of Street Images:   26597\n",
    "Shape of each image:       (1080, 1920, 3)\n",
    "```\n",
    "\n",
    "街の画像数(26597)に比べて、駐車場の画像数(3346)が少ないので、このままで続くとバイアスがありますでしょう。駐車場データの水増しが必要と思いますが、JupyterHub内のデータはread-onlyなので、kerasのImageDataGenerator以外の方法でどうしても水増しができませんでした。ImageDataGeneratorを使ったら、駐車場データだけの水増しではなく、全データセットの水増しになるので、JupyterHubでいい結果が出ないと判断しました。\n",
    "\n",
    "なので、Boxからビデオデータをダウンロードし、自分でffmpegでビデオデータを画像データに変えました。\n",
    "\n",
    "私のパソコンはSurface Pro 6なので、1920x1080の画像データの処理は大変時間が掛かります。以下で1920x1080の画像データを480x270に変え、駐車場の画像のみを水増ししました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 画像データのresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 一回のみ\n",
    "# for image in os.listdir('parking'):\n",
    "#     im = Image.open('parking'+r'/'+image)\n",
    "#     im = im.resize((480,270))\n",
    "#     im.save('resized'+r'/'+image)\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 駐車場画像の水増し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 一回のみ\n",
    "# p = Augmentor.Pipeline(\"train/parking\")\n",
    "# p.flip_left_right(0.5)\n",
    "# p.zoom_random(0.5,0.3)\n",
    "# p.random_distortion(0.5,4,4,8)\n",
    "# p.random_brightness(0.5,0.7,1.3)\n",
    "# p.skew(0.5,0.5)\n",
    "# p.sample(18000)\n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 画像枚数の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像データをもう分けましたので、これをコメントしています。\n",
    "# total_parking=len(os.listdir('data/parking'))\n",
    "# print('Number of Parking Images: {}'.format(total_parking))\n",
    "# total_street=len(os.listdir('data/street'))\n",
    "# print('Number of Street Images: {}'.format(total_street))\n",
    "# total_data=total_parking+total_street\n",
    "# print('Total Images: {}'.format(total_data))\n",
    "\n",
    "total_train = len(os.listdir('data/train/parking')+os.listdir('data/train/street'))\n",
    "total_val = len(os.listdir('data/val/parking')+os.listdir('data/val/street'))\n",
    "total_test = len(os.listdir('data/test/parking')+os.listdir('data/test/street'))\n",
    "total_parking = len(os.listdir('data/train/parking')+os.listdir('data/val/parking')+os.listdir('data/test/parking'))\n",
    "total_street = len(os.listdir('data/train/street')+os.listdir('data/val/street')+os.listdir('data/test/street'))\n",
    "print(\"Number of Parking Images: \",total_parking)\n",
    "print(\"Number of Street Images:  \",total_street)\n",
    "print(\"Total Number of Images:   \",total_parking+total_street)\n",
    "print(\"\")\n",
    "print(\"Number of Train Images:   \",total_train)\n",
    "print(\"Number of Val Images:     \",total_val)\n",
    "print(\"Number of Test Images:    \",total_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今、駐車場の画像数(21544)と街の画像数(21612)は大体同じになりました。\n",
    "\n",
    "```{python}\n",
    "# 試み1\n",
    "directory = \"train/\"\n",
    "images = []\n",
    "labels = np.array([])\n",
    "\n",
    "for label in os.listdir(directory):\n",
    "    if label == 'parking':\n",
    "        code = 0\n",
    "    elif label == 'street':\n",
    "        code = 1\n",
    "\n",
    "    for img in os.listdir(directory+label):\n",
    "        image = cv2.imread(directory+label+r'/'+img)\n",
    "        np.append(images, image, axis=0)\n",
    "        np.append(labels, label, axis=0)\n",
    "```\n",
    "\n",
    "```{python}\n",
    "# 試み２\n",
    "parking_list = list(map(cv2.imread, glob.glob('train/parking/*.png')))\n",
    "street_list = list(map(cv2.imread, glob.glob('train/street/*.png')))\n",
    "X = np.array(parking_list+street_list)\n",
    "y = [1]*len(parking_list)+[0]*len(street_list)\n",
    "```\n",
    "\n",
    "train_test_splitのため、cv2で以上のように読み込んでみましたが、resizeしても四万枚画像を数値arrayに変える事は時間が掛かるので、cross validationができないと判断し、trainsetとtestsetをマニュアル作成しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データセットのマニュアル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 一回のみ\n",
    "# splitfolders.ratio('Data', output=\"Data\", seed=123, ratio=(.8,0.1,0.1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 画像の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory('data/train',\n",
    "                                              batch_size = batch_size,\n",
    "                                              classes = ['parking','street'],\n",
    "                                              shuffle = True,\n",
    "                                              target_size = (h,w),\n",
    "                                              class_mode = 'binary')\n",
    "\n",
    "val_data = test_datagen.flow_from_directory('data/val',\n",
    "                                              batch_size = batch_size,\n",
    "                                              classes = ['parking','street'],\n",
    "                                              shuffle = True,\n",
    "                                              target_size = (h,w),\n",
    "                                              class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 画像の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_imgs, _ = next(train_data)\n",
    "fig, axes = plt.subplots(1,5,figsize=(20,20))\n",
    "axes = axes.flatten()\n",
    "for img, ax in zip(sample_imgs[:5], axes):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手順３：モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(128, 3, activation='relu', input_shape=(h,w,3)),\n",
    "        Dropout(0.2),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(64, 3, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        MaxPooling2D(),\n",
    "        Conv2D(32, 3, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ProgbarLogger' object has no attribute 'log_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-6b77c76379c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                     validation_steps=total_val // batch_size)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\mrq_venv_learn\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m                 \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m                 initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mrq_venv_learn\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mrq_venv_learn\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mrq_venv_learn\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    258\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mrq_venv_learn\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mrq_venv_learn\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    609\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ProgbarLogger' object has no attribute 'log_values'"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    steps_per_epoch=total_train // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=val_data,\n",
    "                    validation_steps=total_val // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
